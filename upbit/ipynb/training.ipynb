{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: imblearn in /Users/yhhan/anaconda3/lib/python3.6/site-packages (0.0)\n",
      "Requirement already satisfied: imbalanced-learn in /Users/yhhan/anaconda3/lib/python3.6/site-packages (from imblearn) (0.5.0)\n",
      "Requirement already satisfied: scipy>=0.17 in /Users/yhhan/anaconda3/lib/python3.6/site-packages (from imbalanced-learn->imblearn) (1.3.0)\n",
      "Requirement already satisfied: scikit-learn>=0.21 in /Users/yhhan/anaconda3/lib/python3.6/site-packages (from imbalanced-learn->imblearn) (0.21.2)\n",
      "Requirement already satisfied: joblib>=0.11 in /Users/yhhan/anaconda3/lib/python3.6/site-packages (from imbalanced-learn->imblearn) (0.13.2)\n",
      "Requirement already satisfied: numpy>=1.11 in /Users/yhhan/anaconda3/lib/python3.6/site-packages (from imbalanced-learn->imblearn) (1.16.4)\n"
     ]
    }
   ],
   "source": [
    "! pip install imblearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sqlite3\n",
    "import os, sys\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import torch\n",
    "import numpy as np\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "import time\n",
    "\n",
    "idx = os.getcwd().index(\"trade\")\n",
    "PROJECT_HOME = os.getcwd()[:idx] + \"trade/\"\n",
    "sys.path.append(PROJECT_HOME)\n",
    "\n",
    "from common.global_variables import *\n",
    "from upbit.upbit_api import Upbit\n",
    "from db.sqlite_handler import *\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_invest_krw(current_price, total_ask_size, total_bid_size):\n",
    "    base_price = current_price * (total_ask_size + total_bid_size) * 0.001\n",
    "    if base_price > 300000:\n",
    "        return 300000\n",
    "    elif 150000 < base_price <= 300000:\n",
    "        return 200000\n",
    "    else:\n",
    "        return 100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_expected_buy_coin_price_for_krw_and_ask_list(ask_price_lst, ask_size_lst, krw, transaction_fee_rate):\n",
    "    original_krw = krw\n",
    "\n",
    "    fee = krw * transaction_fee_rate\n",
    "    krw = krw - fee\n",
    "\n",
    "    calc_size_sum = 0.0\n",
    "\n",
    "    # print(0, krw, calc_size_sum, 0)\n",
    "    for i, ask_size in enumerate(ask_size_lst):\n",
    "        calc_krw_sum = ask_price_lst[i] * ask_size\n",
    "        if calc_krw_sum > krw:\n",
    "            calc_size_sum += krw / ask_price_lst[i]\n",
    "            # print(i+1, krw, calc_size_sum)\n",
    "            break\n",
    "        else:\n",
    "            calc_size_sum += ask_size\n",
    "            krw = krw - calc_krw_sum\n",
    "            # print(i+1, krw, calc_size_sum)\n",
    "\n",
    "    calc_price = (original_krw - fee) / calc_size_sum\n",
    "\n",
    "    # 매수원금: 1000000, 수수료: 500.0, 매수단가: 1823.7691975619496, 확보한 코인수량: 548.0408383561644\n",
    "    return original_krw, fee, calc_price, calc_size_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_expected_sell_coin_price_for_volume_and_bid_list(bid_price_lst, bid_size_lst, volume, transaction_fee_rate):\n",
    "    calc_krw_sum = 0.0\n",
    "    original_volume = volume\n",
    "\n",
    "    #print(0, volume, calc_krw_sum)\n",
    "    for i, bid_size in enumerate(bid_size_lst):\n",
    "        if bid_size > volume:\n",
    "            calc_krw_sum += bid_price_lst[i] * volume\n",
    "            #print(i+1, volume, calc_krw_sum)\n",
    "            break\n",
    "        else:\n",
    "            calc_krw_sum += bid_price_lst[i] * bid_size\n",
    "            volume = volume - bid_size\n",
    "            #print(i+1, volume, calc_krw_sum)\n",
    "\n",
    "    calc_price = calc_krw_sum / original_volume\n",
    "\n",
    "    fee = calc_krw_sum * transaction_fee_rate\n",
    "\n",
    "    calc_krw_sum = calc_krw_sum - fee\n",
    "\n",
    "    # 매도 코인수량: 548.0408383561644, 매도단가: 1805.0, 수수료: 494.79924644171336, 매도결과금:989103.693636985\n",
    "    return original_volume, calc_price, fee, calc_krw_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = select_all_from_order_book_for_one_coin.format(\"BTC\").replace(\"\\n\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_timeseries(data, data_normalized, window_size, future_target_size, up_rate):\n",
    "    future_target = future_target_size - 1\n",
    "\n",
    "    dim_0 = data.shape[0] - window_size - future_target\n",
    "    dim_1 = data.shape[1]\n",
    "\n",
    "    x = torch.zeros((dim_0, window_size, dim_1)).to(DEVICE)\n",
    "    x_normalized = torch.zeros((dim_0, window_size, dim_1)).to(DEVICE)\n",
    "\n",
    "    y = torch.zeros((dim_0,)).to(DEVICE)\n",
    "    y_up = torch.zeros((dim_0,)).float().to(DEVICE)\n",
    "\n",
    "    for i in range(dim_0):\n",
    "        x[i] = data[i: i + window_size]\n",
    "        x_normalized[i] = data_normalized[i: i + window_size]\n",
    "\n",
    "    count_one = 0\n",
    "    for i in range(dim_0):\n",
    "        max_price = -1.0\n",
    "\n",
    "        ask_price_lst = []\n",
    "        ask_size_lst = []\n",
    "        for w in range(0, 60, 4):\n",
    "            ask_price_lst.append(x[i][-1][1 + w].item())\n",
    "            ask_size_lst.append(x[i][-1][3 + w].item())\n",
    "\n",
    "        invest_krw = get_invest_krw(\n",
    "            current_price=x[i][-1][1].item(),\n",
    "            total_ask_size=x[i][-1][121],\n",
    "            total_bid_size=x[i][-1][123]\n",
    "        )\n",
    "\n",
    "        original_krw, fee, calc_price, calc_size_sum = get_expected_buy_coin_price_for_krw_and_ask_list(\n",
    "            ask_price_lst=ask_price_lst,\n",
    "            ask_size_lst=ask_size_lst,\n",
    "            krw=invest_krw,\n",
    "            transaction_fee_rate=TRANSACTION_FEE_RATE\n",
    "        )\n",
    "\n",
    "        for j in range(future_target + 1):\n",
    "            bid_price_lst = []\n",
    "            bid_size_lst = []\n",
    "            for w in range(0, 60, 4):\n",
    "                bid_price_lst.append(data[i + window_size + j][61 + w].item())\n",
    "                bid_size_lst.append(data[i + window_size + j][63 + w].item())\n",
    "\n",
    "            original_volume, future_price, fee, future_krw_sum = get_expected_sell_coin_price_for_volume_and_bid_list(\n",
    "                bid_price_lst=bid_price_lst,\n",
    "                bid_size_lst=bid_size_lst,\n",
    "                volume=calc_size_sum,\n",
    "                transaction_fee_rate=TRANSACTION_FEE_RATE\n",
    "            )\n",
    "\n",
    "            if future_price > max_price:\n",
    "                max_price = future_price\n",
    "\n",
    "        y[i] = max_price\n",
    "\n",
    "        if y[i] > calc_price * (1 + up_rate):\n",
    "            y_up[i] = 1\n",
    "            count_one += 1\n",
    "\n",
    "    return x, x_normalized, y, y_up, count_one / dim_0, dim_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "coin_name = \"ADA\"\n",
    "sqlite3_order_book_db_filename = PROJECT_HOME + \"db/upbit_order_book_info.db\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(data_length, split=True):\n",
    "    df = pd.read_sql_query(\n",
    "        select_all_from_order_book_for_one_coin.format(coin_name),\n",
    "        sqlite3.connect(sqlite3_order_book_db_filename, timeout=10, check_same_thread=False)\n",
    "    )\n",
    "\n",
    "    df = df.drop([\"base_datetime\", \"collect_timestamp\"], axis=1)[:data_length]\n",
    "    \n",
    "    data = torch.from_numpy(df.values).to(DEVICE)\n",
    "\n",
    "    min_max_scaler = MinMaxScaler()\n",
    "    data_normalized = min_max_scaler.fit_transform(df.values)\n",
    "    data_normalized = torch.from_numpy(data_normalized).to(DEVICE)\n",
    "\n",
    "    x, x_normalized, y, y_up, one_rate, total_size = build_timeseries(\n",
    "        data=data,\n",
    "        data_normalized=data_normalized,\n",
    "        window_size=WINDOW_SIZE,\n",
    "        future_target_size=FUTURE_TARGET_SIZE,\n",
    "        up_rate=UP_RATE\n",
    "    )\n",
    "    \n",
    "    print(one_rate, total_size)\n",
    "    \n",
    "    # Imbalanced Preprocessing - Start\n",
    "    if one_rate > 0.01:\n",
    "        x_normalized = x_normalized.cpu()\n",
    "        y_up = y_up.cpu()\n",
    "\n",
    "        try:\n",
    "            x_samp, y_up_samp = RandomUnderSampler(sampling_strategy=0.75).fit_sample(\n",
    "                x_normalized.reshape((x_normalized.shape[0], x_normalized.shape[1] * x_normalized.shape[2])),\n",
    "                y_up\n",
    "            )\n",
    "            x_normalized = torch.from_numpy(\n",
    "                x_samp.reshape(x_samp.shape[0], x_normalized.shape[1], x_normalized.shape[2])\n",
    "            ).to(DEVICE)\n",
    "            y_up = torch.from_numpy(y_up_samp).to(DEVICE)\n",
    "        except ValueError:\n",
    "            logger.info(\"{0} - {1}\".format(coin_name, \"RandomUnderSampler - ValueError\"))\n",
    "            x_normalized = x_normalized.to(DEVICE)\n",
    "            y_up = y_up.to(DEVICE)\n",
    "    # Imbalanced Preprocessing - End\n",
    "    \n",
    "    total_size = len(x_normalized)\n",
    "        \n",
    "    if split:\n",
    "\n",
    "        indices = list(range(total_size))\n",
    "        np.random.shuffle(indices)\n",
    "\n",
    "        train_indices = list(set(indices[:int(total_size * 0.8)]))\n",
    "        validation_indices = list(set(range(total_size)) - set(train_indices))\n",
    "\n",
    "        x_train_normalized = x_normalized[train_indices]\n",
    "        x_valid_normalized = x_normalized[validation_indices]\n",
    "\n",
    "        y_up_train = y_up[train_indices]\n",
    "        y_up_valid = y_up[validation_indices]\n",
    "\n",
    "        one_rate_train = y_up_train.sum().float() / y_up_train.size(0)\n",
    "        one_rate_valid = y_up_valid.sum().float() / y_up_valid.size(0)\n",
    "\n",
    "        train_size = x_train_normalized.size(0)\n",
    "        valid_size = x_valid_normalized.size(0)\n",
    "\n",
    "        return x_train_normalized, y_up_train, one_rate_train, train_size,\\\n",
    "               x_valid_normalized, y_up_valid, one_rate_valid, valid_size\n",
    "    else:\n",
    "        one_rate = y_up.sum().float() / y_up.size(0)\n",
    "        return x_normalized, y_up, one_rate, total_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "\n",
    "def get_best_model_by_nested_cv(X, y, inner_cv, outer_cv, Classifier, parameter_grid):\n",
    "    outer_score_list = []\n",
    "    best_param_list = []\n",
    "    model_list = []\n",
    "    \n",
    "    num_outer_split = 1\n",
    "    for training_samples_idx, test_samples_idx in outer_cv.split(X, y):\n",
    "        print(\"[Outer Split: #{0}]\".format(num_outer_split))\n",
    "        best_parms = {}\n",
    "        best_score = -np.inf\n",
    "\n",
    "        for parameters in parameter_grid:\n",
    "#             print(\"Parameters: {0}\".format(parameters))\n",
    "            cv_scores = []\n",
    "            num_inner_split = 1\n",
    "            for inner_train_idx, inner_test_idx in inner_cv.split(X[training_samples_idx], y[training_samples_idx]):\n",
    "                clf = Classifier(**parameters)\n",
    "                clf.fit(X[inner_train_idx], y[inner_train_idx])\n",
    "                score = clf.score(X[inner_test_idx], y[inner_test_idx])\n",
    "                \n",
    "                cv_scores.append(score)\n",
    "#                 print(\"Inner Split: #{0}, Score: #{1}\".format(\n",
    "#                     num_inner_split,\n",
    "#                     score\n",
    "#                 ))\n",
    "                num_inner_split += 1\n",
    "\n",
    "            mean_score = np.mean(cv_scores)\n",
    "            if mean_score > best_score:\n",
    "                best_score = mean_score\n",
    "                best_params = parameters\n",
    "#             print(\"Mean Score:{0}, Best Score:{1}\".format(mean_score, best_score))\n",
    "\n",
    "        print(\"* Outer Split: #{0}, Best Score: {1}, Best Parameter: #{2} ***\\n\".format(\n",
    "            num_outer_split,\n",
    "            best_score,\n",
    "            best_params\n",
    "        ))\n",
    "\n",
    "        clf = Classifier(**best_params)\n",
    "        clf.fit(X[training_samples_idx], y[training_samples_idx])\n",
    "\n",
    "        best_param_list.append(best_params)\n",
    "        outer_score_list.append(clf.score(X[test_samples_idx], y[test_samples_idx]))\n",
    "        model_list.append(clf)\n",
    "        \n",
    "        num_outer_split += 1\n",
    "\n",
    "    best_score = -np.inf\n",
    "    best_model = None\n",
    "    for idx, score in enumerate(outer_score_list):        \n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_model = model_list[idx]\n",
    "\n",
    "    return best_score, best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.13258426966292136 445\n",
      "x_normalized_original: torch.Size([137, 18, 125]), y_up_original: torch.Size([137]), one_rate: 0.43065693974494934, total_size: 137\n"
     ]
    }
   ],
   "source": [
    "DATA_LENGTH = 480\n",
    "\n",
    "x_normalized_original, y_up_original, one_rate, total_size = get_dataset(DATA_LENGTH, split=False)\n",
    "\n",
    "print(\"x_normalized_original: {0}, y_up_original: {1}, one_rate: {2}, total_size: {3}\".format(\n",
    "    x_normalized_original.size(),\n",
    "    y_up_original.size(),\n",
    "    one_rate,\n",
    "    total_size\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_sklearn_model(coin_name, x_normalized_original, y_up_original, total_size, one_rate):    \n",
    "    param_grid = {\n",
    "        'learning_rate': [0.01, 0.05, 0.1],\n",
    "        'max_depth': np.linspace(1, 9, 4, endpoint=True),\n",
    "        'n_estimators': [32, 64, 128],\n",
    "        'max_features': list(range(int(x_normalized_original.shape[1] / 2), x_normalized_original.shape[1], 2)),\n",
    "    }\n",
    "        \n",
    "    is_high_quality = False\n",
    "\n",
    "    batch_size = 16\n",
    "    patience = 30\n",
    "\n",
    "    coin_model_start_time = time.time()\n",
    "    \n",
    "    X = x_normalized_original.numpy().reshape(total_size, -1)\n",
    "    y = y_up_original.numpy()\n",
    "    \n",
    "#     print(\"X.shape: {0}\".format(X.shape))\n",
    "#     print(\"y.shape: {0}\".format(y.shape))\n",
    "\n",
    "    best_score, best_model = get_best_model_by_nested_cv(\n",
    "        X=X,\n",
    "        y=y,\n",
    "        inner_cv=StratifiedKFold(n_splits=4, shuffle=True),\n",
    "        outer_cv=StratifiedKFold(n_splits=4, shuffle=True),\n",
    "        Classifier=GradientBoostingClassifier,\n",
    "        parameter_grid=ParameterGrid(param_grid)\n",
    "    )\n",
    "    \n",
    "    print(best_model)\n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset_for_buy(coin_name, model_type=\"LSTM\"):\n",
    "    df = pd.read_sql_query(\n",
    "        select_all_from_order_book_for_one_coin_recent_window.format(coin_name, WINDOW_SIZE),\n",
    "        sqlite3.connect(sqlite3_order_book_db_filename, timeout=10, check_same_thread=False)\n",
    "    )\n",
    "\n",
    "    df = df.sort_values(['collect_timestamp', 'base_datetime'], ascending=True)\n",
    "    df = df.drop([\"base_datetime\", \"collect_timestamp\"], axis=1)\n",
    "\n",
    "    min_max_scaler = MinMaxScaler()\n",
    "    data_normalized = min_max_scaler.fit_transform(df.values)\n",
    "    data_normalized = torch.from_numpy(data_normalized).float().to(DEVICE)\n",
    "    \n",
    "    if model_type == \"LSTM\":\n",
    "        return data_normalized.unsqueeze(dim=0)\n",
    "    else:\n",
    "        data_normalized = data_normalized.flatten()\n",
    "        return data_normalized.unsqueeze(dim=0)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 18, 125])\n"
     ]
    }
   ],
   "source": [
    "X_prediction = get_dataset_for_buy(\"BTC\")\n",
    "print(X_prediction.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*************** QKC\n",
      "[Outer Split: #1]\n",
      "* Outer Split: #1, Best Score: 0.9703846153846154, Best Parameter: #{'learning_rate': 0.01, 'max_depth': 3.6666666666666665, 'max_features': 13, 'n_estimators': 128} ***\n",
      "\n",
      "[Outer Split: #2]\n",
      "* Outer Split: #2, Best Score: 0.97, Best Parameter: #{'learning_rate': 0.1, 'max_depth': 1.0, 'max_features': 11, 'n_estimators': 32} ***\n",
      "\n",
      "[Outer Split: #3]\n",
      "* Outer Split: #3, Best Score: 0.9615384615384616, Best Parameter: #{'learning_rate': 0.1, 'max_depth': 1.0, 'max_features': 9, 'n_estimators': 128} ***\n",
      "\n",
      "[Outer Split: #4]\n",
      "* Outer Split: #4, Best Score: 0.9518660968660968, Best Parameter: #{'learning_rate': 0.1, 'max_depth': 3.6666666666666665, 'max_features': 17, 'n_estimators': 32} ***\n",
      "\n",
      "GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
      "                           learning_rate=0.1, loss='deviance', max_depth=1.0,\n",
      "                           max_features=11, max_leaf_nodes=None,\n",
      "                           min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                           min_samples_leaf=1, min_samples_split=2,\n",
      "                           min_weight_fraction_leaf=0.0, n_estimators=32,\n",
      "                           n_iter_no_change=None, presort='auto',\n",
      "                           random_state=None, subsample=1.0, tol=0.0001,\n",
      "                           validation_fraction=0.1, verbose=0,\n",
      "                           warm_start=False)\n",
      "[[0.52190938 0.47809062]]\n",
      "*************** GRS\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 308250 into shape (4137,newaxis)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-9b9bcac631f8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcoin_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mupbit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_all_coin_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"***************\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoin_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mbest_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_sklearn_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcoin_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_normalized_original\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_up_original\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mone_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mX_prediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_dataset_for_buy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcoin_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-6529e55c5b14>\u001b[0m in \u001b[0;36mmake_sklearn_model\u001b[0;34m(coin_name, x_normalized_original, y_up_original, total_size, one_rate)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mcoin_model_start_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_normalized_original\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_up_original\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: cannot reshape array of size 308250 into shape (4137,newaxis)"
     ]
    }
   ],
   "source": [
    "upbit = Upbit(CLIENT_ID_UPBIT, CLIENT_SECRET_UPBIT, fmt)\n",
    "for coin_name in upbit.get_all_coin_names():\n",
    "    print(\"***************\", coin_name)\n",
    "    best_model = make_sklearn_model(coin_name, x_normalized_original, y_up_original, total_size, one_rate)\n",
    "    \n",
    "    X_prediction = get_dataset_for_buy(coin_name)\n",
    "    \n",
    "    total_size = X_prediction.size(0)\n",
    "    X_prediction = X_prediction.view(total_size, -1)\n",
    "    y_prediction = best_model.predict_proba(X_prediction[-1].unsqueeze(dim=0))\n",
    "\n",
    "    print(y_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
